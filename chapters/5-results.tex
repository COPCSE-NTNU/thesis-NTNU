\chapter{Results}
\label{chap:results}

In this section, the results from the proposed solution is described in detail. It describes different results from the different stages throughout the develop process and presents the final results and metrics from the trained \acrfull{ml} model.

\section{Constructed dataset and ML problem formulation}

The initial dataset was copied and validated from \acrfull{mo}'s \acrshort{ais} database. The database table \textit{``vessel\_positions\_history''} was last updated in March 2021 and consists of \textbf{1.2} billion positional \acrshort{ais} records. Each vessel that transmitted positions belongs to a given segment and sub-segment that was made available by the \textit{``vessel\_segment''} table which contains \textbf{eight} different segment values, and \textbf{107} different combinations of segments and sub-segments. The provided \textit{``ports''} data contains \textbf{5200} ports world-wide that all follows the \gls{locode} naming standard. In total, as of March 2021, there were \textbf{6.4} million vessel transitions in the \textit{``vessel\_transitions''} table which was used to construct voyages. This data formed the initial data foundation for the final processed \acrfull{ml} training dataset. All the data that was copied and processed from \acrshort{mo}'s databases were processed in batches. Ports, segments, and transitions where quickly copied and processed, however, the \textbf{1.2} billion positional records took several days to migrate and validate. This was mostly because of the time required to validate coordinates and correctly map \acrshort{mmsi} and \acrshort{imo} values. Throughout this process, the latest identifiers and timestamps were fetched from the dedicated project database to only update data that occurred after the latest records already processed. In this way, this process was idempotent so that running the process multiple times did not affect existing data. This made the system simple to update throughout the development process and as many records as possible were used in the final approach only limited by the thesis time limitation.

\subsection{Voyage definition and construction}

Based on the initial \textbf{6.4} million vessel transitions, \textbf{1.7} million voyages where initially constructed by finding positional records transmitted from a vessel between subsequent departure and arrival transitions. The resulting voyages were, therefore, defined based on transitioning \acrshort{ais} statuses that indicate the vessel is moored or moving. As a consequence of this definition, the quality of the resulting trajectories are very much affected by how well the \gls{aivdm} protocol is followed by the traveling vessels. Since the navigational status attribute is manually inputted by the vessel's captain or crew, the resulting trajectories are prone to human error but results in more complete voyages disregarding intermediate stops for purposes such as bunkering.

As an example, \cref{fig:transition_voyage} shows a voyage from China to Argentina where the vessel stopped at Singapore, most likely to bunker. In the chosen voyage definition, the beginning and end of the voyage is defined based on input from the vessel's captain which results in a voyage starting from China, and ending in Argentina. Further implications and consequences of the chosen definition is later discussed in \cref{chap:discussion}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/results/voyage_quality}
    \caption{Transition voyage from China to Argentina that visits the port of Singapore exemplifying the properties of the chosen voyage definition.}
    \label{fig:transition_voyage}
\end{figure}

The \textbf{1.7} million voyages constructed using the vessel transitions were sampled based on 6 hour intervals and collected in ``sampled\_transition\_voyages'' that formed the foundation for trajectory similarity measurements. In the process of constructing the final dataset, these sampled voyages were divided into multiple incomplete voyages up to a factor of four. The resulting training dataset collected in the table ``ml\_training\_data'' consisted of \textbf{4.3} million voyages.

\subsection{Trajectory similarity and MSTD}

Using the foundation of the sampled trajectories, each trajectory was compared to every other trajectory departing the same port to calculate the \acrfull{mstd}. The \acrshort{mstd} value was used primarily as a method of abstracting geographical trajectories into categorical and numerical values that a \acrfull{ml} model could work with. This process converted a voyage's geographical trajectory into MSTD, the similarity value to the most similar trajectory, and trajectory length. Thus, the MSTD value served as an initial prediction purely based on geographical trajectory similarity measurements using \acrfull{sspd}. The \acrshort{sspd} method was chosen for its ability to effectively handle different lengths and shapes of trajectories when estimating similarity. Furthermore, in the approach proposed in \cite{Zhang2020AISApproach} the \acrshort{sspd} method performed the best out of the algorithmic approaches evaluated, although, their own Random Forest (RF) based approach performed the best. However, the way the training data is structured, the trajectory similarity method of choice is completely interchangeable with others. The only requirement for a given trajectory similarity measurement is that it also produces a similarity value that serves as a weight for the \acrshort{mstd} value.

\acrshort{mstd} as an initial prediction seemed to be a decent initial indicator as to where the vessel would be arriving. In total, there were \textbf{4 306 271} entries in the final training data generated where exactly \textbf{1 423 476} of which has the same arrival port and \acrshort{mstd} value. Thus, it can be assumed that the purely spatial prediction using incomplete sampled historical voyages based on \acrshort{sspd} was \textit{33\%} accurate. In other words, when using an algorithmic prediction approach based on purely spatial trajectory similarity measurements, voyage destinations can be predicted correctly one third of the time. This formed a baseline accuracy to beat with the \acrshort{ml}-based solution.

\subsection{ML data preparation}

After the final training dataset was built, it was discovered that in terms of arrival port frequencies, the dataset was imbalanced thus making it harder for \acrshort{ml} models to learn. Although some models can better handle dataset imbalance, a sampling approach was used to balance the dataset before training to support different ML models. Several different sampling approaches were evaluated, however, the traditional over and under -sampling methods either produced massive amounts of synthetic data, or removed almost all the original data which was shown in \cref{fig:all_samplers} in \cref{chap:method}. Thus, an ensemble sampling method of majority undersampling and ``SMOTE+ENN'' was employed to balance the dataset before training. \cref{fig:ensemble_sampler} shows the results from the ensemble sampling method that uses a combination of under and over -sampling techniques. As \cref{fig:ensemble_sampler} shows, using a subset of the full dataset, the final result is 8\% smaller than the original dataset, is a lot more balanced, but still has differences in class frequencies which persisted from the original dataset.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/imbalance/ensemble}
    \caption{Final ensemble sampling method (right) compared to original dataset (left) where the final ensemble produces a dataset similar in size to that of the original.}
    \label{fig:ensemble_sampler}
\end{figure}

\section{Model training and prediction performance}

After the training dataset was constructed, encoded, and balanced, the model was trained using different approaches as described in \cref{sec:training_process}. This section describes the results from the training processes, the final approach used, and the resulting model's performance and predictions resulting from the evaluation process described in \cref{sec:evaluation_process}.

\subsection{Training process}

As described in \cref{sec:training_process}, multiple training processes were evaluated in order to find the most appropriate method of training a larger model on an extensive dataset. For the \acrfull{xgb} model, three different training processes were evaluated in this process.

First, the iterative approach was evaluated by training the model in batches of \textit{600 000} samples at the time. This approach seemed to work as intended, however, it was discovered that during subsequent training batches, the performance of the model dropped off for each iteration. It seemed as if the model did not handle continuous training of the same model as well as it does when training one model from scratch using the complete dataset. Furthermore, the parameter \textit{``early\_stopping\_rounds''} was used in the other approaches as a method of telling the model to stop training if it does not see any improvements after the given number of rounds. When this parameter is set using the iterative approach, the model can stop producing new trees before it has constructed the total number of trees allowed by the \textit{``n\_estimators''} parameter. Since the first iteration can produce a model with less trees than allowed, the next iteration fails as the number of allowed trees does not match with the previous model's actual number of trees. Although there are ways around this issue, as using the early stopping rounds parameter is useful to avoid overfitting, the iterative approach did not seem the most appropriate during the development process.

Next, it was attempted to train the model using the external memory, or ``out-of-core'' memory version of \acrshort{xgb}. In this approach, the \acrshort{xgb} library is provided a \textit{libsvm} file which it converts to an optimized matrix format which is kept on the computers file system. However, all attempts at training the model using external memory were unsuccessful as the training process consumed all of the running computer's available memory and resulted in a ``bad allocation`` memory error. There seems to either be a misconfiguration or an underlying issue with the Python library used in the implementation. However, since the expected results from this approach should be the same as training the model in one iteration on a capable computer, these issues were not further looked into, although, it could be beneficial to reduce the resource requirements for the training process for future use. Therefore, it could warrant more investigation for future work.

Finally, the entire dataset was used to train the final model in one iteration on a computer capable of running the process. The training process ran over the course of two days and consistently required around 200GB of memory. The vast memory consumption could be somewhat reduced by not evaluating the model during the training process which is appropriate for future training processes after the model has been trained and the training configuration has been validated. As described in \cref{sec:training_process}, an extra copy of the training and test datasets were kept in memory to continuously evaluate and monitor the training process.

\subsection{Performance}

During the training process, the performance of the model was continuously evaluated to measure logarithmic loss and multi-class classification error. \cref{fig:eval_set} shows these metrics plotted over each boosting round in the training process. Both graphs starts converging at 100 decision trees have been constructed at around \textbf{1.5} log loss, and around \textbf{0.3} classification error. This corresponds to around \textbf{70\%} accuracy. Since the graphs have not completely converged, it is possible to either increase the learning rate parameter or increase the number of estimators in the tree, although it seems as if the graphs are very close to converging, so it might not increase performance noticeably and increases risk of overfitting. As there is very little difference between the performance on the training set and evaluation set, it indicates that the model is not overfitting, however, it might indicate that the model is over optimistic. This could occur when there are several similar samples in the training and the test datasets and could be a consequence of the sampling techniques used to balance the dataset.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/results/eval_set}
    \caption{Logarithmic loss and classification error metrics tracked per boosting round in the training process.}
    \label{fig:eval_set}
\end{figure}

After the training process finished, the test dataset was used to make predictions to further evaluate the results. From the resulting predictions, accuracy was calculated to be \textbf{72\%}, and a class report was generated that shows more metrics for each possible class, or encoded arrival port, that might provide more insight into the model's performance than accuracy. \cref{lst:class_report} shows a summarized output from this class report showing the metrics precision, recall, f1-score, and support for each class as well as the aggregated mean values from all of the classes. As mentioned in \cref{sec:model_evaluation}, f1-score is based on precision and recall and is particularly appropriate for measuring performance on imbalanced datasets, and as \cref{lst:class_report} shows, the f1-score does not deviate much from the estimated accuracy of \textbf{72\%}, or \textbf{0.72}. This indicates that the accuracy value is reliable and is not biased by dataset imbalance.

\begin{lstlisting}[
    caption={Class report based on prediction results from the test dataset. The performance of the classifier is evaluated per class by using precition, recall, f1-score, and support.},
    label=lst:class_report,
    showstringspaces=false,
    basicstyle=\ttfamily,
]
[XGBoostClassifier] Class Report:
             precision    recall  f1-score   support      pred
0             0.378049  0.240310  0.293839     258.0     164.0
1             0.816850  0.810909  0.813869     275.0     273.0
2             0.722222  0.541667  0.619048     312.0     234.0
3             0.672727  0.377551  0.483660     294.0     165.0
...           ...       ...       ...          ...       ...
3067          0.824675  0.849498  0.836903     299.0     308.0
3068          0.833922  0.778878  0.805461     303.0     283.0
3069          0.773050  0.762238  0.767606     286.0     282.0
3070          0.614035  0.557325  0.584307     314.0     285.0
...           ...       ...       ...          ...       ...
avg / total   0.718698  0.715150  0.712737  878049.0  878049.0

\end{lstlisting}

Lastly, in order to ensure the model is not overfitted, a three-fold cross validation process was employed. \cref{lst:cv_result} shows the results from the three folds that the model was trained on. It is recommended, or common to use more folds ranging from five to 10, however, because of the long training time and time limitations, only three folds were used. As described in \cref{sec:model_evaluation}, since the standard deviation (noted as ``std. dev.'' in \cref{lst:cv_result}) is low, the model is likely to not be overfitted.

\begin{lstlisting}[
    caption={Output from 3-fold cross validation. \todo{update numbers}},
    label=lst:cv_result,
    showstringspaces=false,
    basicstyle=\ttfamily,
]
 Folds:      [0.70569399 0.71297481 0.72244745]
 Mean:       0.7137054183485277
 Std. dev.:  0.006859056778768982
\end{lstlisting}

\section{Prediction results}

After the model was trained and evaluated, \textit{20\%} of the total training dataset was used to evaluate the model. This evaluation process resulted in around \textit{880 000} example predictions. These predictions were further analyzed to discuss the impact and meaning of the different features used in the dataset. These results are presented in this section.

\subsection{Feature importances}

An added benefit of using a tree based model such as the \acrfull{xgb} or \acrfull{rf} model is that they can provide insight into the importances of features, or attributes. In a decision tree based ensemble, when constructing a tree, the training data is analyzed to find the best features to make splits, or branches, in the trees. After the training process, the models can then produce a ranking over what features best divided the dataset best. This is referred to as feature importance.

\begin{table}[htbp]
    \centering
    \begin{tabularx}{0.6\textwidth}{X X}
        \bfseries{Feature} & \bfseries{Importance} \\ \toprule
        sspd\_mstd         & 0.443659 \\ \midrule
        departure\_port    & 0.226288 \\ \midrule
        segmentation       & 0.180907 \\ \midrule
        sspd\_dist         & 0.083816 \\ \midrule
        trajectory\_length & 0.065331 \\ \bottomrule
    \end{tabularx}
    \caption{Feature importances based on the \acrshort{xgb} decision tree ensemble process}\label{tab:feature_importances}
\end{table}

\cref{tab:feature_importances} shows an overview of the produced feature importances after the \acrshort{xgb} training process. As it shows, the most important feature was the \acrshort{mstd} value at a ranking of 0.44 out of 1.0, followed by the vessel's departure port, segmentation value, and then the similarity value and voyage length. This analysis can further help decide if features are worth dropping from the dataset, and insight into what attributes are good indicators during voyage predictions. As mentioned in \cref{sec:dataset_imbalance}, the attributes ``segment'', and ``sub-segment'' were combined into one segmentation value in order to ensure that no invalid segment and sub-segment combinations could be generated by sampling methods. A disadvantage of this is that the feature importances of the two attributes are lost in favor of the combined value. However, from test runs made during the development process with and without sampling, the importance of segment and sub-segment were usually ranked where segmentation is in \cref{tab:feature_importances} with sub-segment being more important than segment.

Furthermore, the results from test dataset predictions were analyzed to find the impact of the attributes that mostly served as weights for the \acrshort{mstd} value, namely, the similarity value (\textit{sspd\_dist}) and trajectory length. \cref{lst:dist_length_impact} shows an output from the evaluation process which shows that the distance value was smaller, on average, for correct predictions while trajectory length did not considerably differ from correct and incorrect predictions. It makes sense that the distance, or similarity, value is lower for correct predictions as the more similar the most similar historical trajectory is, the more valuable the \acrshort{mstd} value is. For instance, if a voyage's most similar historical trajectory has a \textit{sspd\_dist} of 0, it is following an exact path of a previous voyage. In this case, the similarity value for correct predictions was on average around \textit{43\%} lower than for incorrect predictions. For the trajectory length, it would make sense that the longer the voyage had traveled, the easier it would be to predict its destination, thus, the length should be longer for correct predictions. However, this is not the case for these predictions. This could be explained by the fact that shorter voyages might be easier to predict than long voyages for small vessels. For instance, it is presumable that, passenger vessels with very short but frequent trajectories are very easy to predict thus bringing the average length down for correct predictions. To confirm this hypothesis, further investigation into the specific segments and sub-segments is required.

\begin{lstlisting}[
    caption={Mean values of similarity value and trajectory length for correct and incorrect predictions.},
    label=lst:dist_length_impact,
    showstringspaces=false,
    tabsize=1,
    basicstyle=\ttfamily,
]
mean ssp_dist for correct predictions:           115642.48170757179
mean trajectory_length for correct predictions:  17.662729492637958

mean ssp_dist for erroneus predictions:           201713.174255885
mean trajectory_length for erroneus predictions:  18.841843522761508
\end{lstlisting}

\subsection{Segment predictability}

As it relates to research question 2 (\cref{sec:research_questions}), the \textit{880 000} predictions from the test dataset were further analyzed in search of patterns in predictability of different types of vessels. These results also serves to gain further insight into the value of the performance metrics. \cref{fig:segment_accuracy} shows a bar chart of the initial accuracy of predictions per segment, and it shows that there are some differences in accuracy per segment overall, but the most of the segments have a similar level of predictability. For example, vessels of the segment ``other'' were the easiest to predict and had the highest accuracy of \textit{76\%}. This is likely to be caused by different types of passenger vessels that lie within this segment. These vessels produce many predictable voyages as they travel between a few number of ports with a high frequency. Furthermore, the ``other'' segment also include very specialized vessels that are limited in terms of possible destination ports.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results/segment_accuracy_new}
    \caption{Accuracy of predictions from test set per segment.}
    \label{fig:segment_accuracy}
\end{figure}

As \cref{fig:other_accuracy} shows, and as expected, the accuracy of the passenger related sub-segments were very high. Since these are so high in frequency and has shorter trajectories, they may be the main cause that the average trajectory length was lower for correct predictions than incorrect ones. On the other hand, container and car ``roll on/roll off'' (roro) vessels travel longer distances less frequently but were also quite predictable. Another segment that could affect the average trajectory length and similarity values for correct predictions is the oil service segment. The oil service vessels should be easy to predict as these vessels travel to oil platforms and often back to the same or another nearby port. However, for these vessels, their trajectories would have been harder to consider as they often do not use the ``moored'' AIS navigational status when arriving at oil platforms. This can lead to very long trajectories that are hard to compare to others, therefore, these vessels should rely more on the departure port rather than the \acrshort{mstd} related values. In general, the ``other'' and ``oil service'' segments are not very relevant for \acrfull{mo}'s customers or for bigger actors in the industry, however, some sub-segments such as container and other general cargo ships can be quite relevant.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/results/seg_other_acc}
    \caption{Accuracy of predictions per sub-segment within the ``other'' segment.}
    \label{fig:other_accuracy}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results/seg_dry_bulk_acc}
    \caption{Accuracy of predictions per sub-segment within the ``dry\_bulk'' segment.}
    \label{fig:dry_bulk_accuracy}
\end{figure}


The dry bulk cargo industry, on the other hand, has more commercial interest. \cref{fig:dry_bulk_accuracy} shows the accuracy per sub-segment for the dry bulk cargo segment. The dry bulk sub-segments are based on the vessels' cargo capacities and sizes, however, as \cref{fig:dry_bulk_accuracy} shows, there seems to be little correlation between vessel size and accuracy. The two most accurately predicted sub-segments are large vessels, however, they are followed closely by the smaller sub-segments, and the two least predictable types are some of the largest. Thus, the uniqueness of the sub-segment value itself had more impact on predictions than the implied size and capacity of the vessels.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/results/seg_lpg_acc}
    \caption{Accuracy of predictions per sub-segment within the ``LPG'' segment.}
    \label{fig:lpg_accuracy}
\end{figure}

The prediction results for tanker sub-segments show similar results as to the dry bulk ones, however, some other segments does seem to show that size and capacity indeed might be correlated to predictability in different ways. For instance, in the chemical segment the two largest sub-segments have the highest accuracies of \textit{90\%} and \textit{85\%}, however, the remaining sub-segments does not show much difference correlated to size. There seem to be a slight correlation in chemical vessels that show that larger vessels are easier to predict than smaller ones, however, for other segments the opposite correlation seems to occur. The \acrfull{lng} and \acrfull{lpg} vessels have the highest correlation between size and accuracy, but in the opposite direction compared to the chemical vessels. \cref{fig:lpg_accuracy} shows that the three smallest \acrshort{lpg} sub-segments \textit{coaster}, \textit{handy}, and \textit{MGC} have the highest accuracy, while the two largest sub-segments \textit{VLGC} and \textit{LGC}  have lower accuracies. This is similar to that of the \acrshort{lng} vessels (\cref{fig:lng_accuracy}) where the largest sub-segments \textit{QMax}, and \textit{QFlex} are harder to predict than the smaller sub-segments.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/results/seg_lng_acc}
    \caption{Accuracy of predictions per sub-segment within the ``LNG'' segment.}
    \label{fig:lng_accuracy}
\end{figure}

Another interesting segment to analyze is the combo segment. These combination vessels can serve multiple functions in that they can carry different types of cargoes. In \cref{fig:segment_accuracy}, the combo segment showed a mid-range general accuracy level, however, when looking into the sub-segments, there are substantial differences in accuracies across the different types of combo vessels (\cref{fig:combo_accuracy}). The ``Klaveness Combination Carriers'' (CABU) and ``Oil-Bulk-Ore'' (OBO) vessels have the highest accuracies. However, there are only 12 CABU vessels and 5 OBO vessels in the world, or in \acrfull{mo}'s vessel database. On the other hand, there are 4700 chemical product tankers in the world that were also quite predictable. These vessels drive the general accuracy of the combo vessels up in \cref{fig:segment_accuracy} as the remaining sub-segments have substantially lower accuracies. It does, however, make sense that combo vessels are generally difficult to predict as they serve multiple functions which results in them having more possible destination ports they can load and unload at.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/results/seg_combo_acc}
    \caption{Accuracy of predictions per sub-segment within the ``combo'' segment.}
    \label{fig:combo_accuracy}
\end{figure}

In conclusion, there seems to be some correlation between vessel size, capacity and predictability. However, this only seems to be the case for some segments while for others, the specific sub-segment value is the more important factor than the implied size and capacity.

\section{Applications and validity}

\begin{itemize}
    \item How to use? (read model from file and predict)
    \item How to train?
    \item Stand-alone usability
\end{itemize}

% \subsection{Integration with \acrfull{mo}}

\subsection{Expert validation}

\begin{itemize}
    \item Caveats/use-cases and value
    \item Evaluation from MO
    \item Evaluation from external actors
\end{itemize}
