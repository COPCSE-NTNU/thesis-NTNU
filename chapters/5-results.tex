\chapter{Results}
\label{chap:results}

In this section, the results from the proposed solution is described in detail. It describes different results from the different stages throughout the develop process and presents the final results and metrics from the trained \acrfull{ml} model.

\section{Constructed dataset and ML problem formulation}

The initial dataset was copied and validated from \acrfull{mo}'s \acrshort{ais} database. The database table \textit{``vessel\_positions\_history''} was last updated in March 2021 and consists of \textbf{1.2} billion positional \acrshort{ais} records. Each vessel that transmitted positions belongs to a given segment and sub-segment that was made available by the \textit{``vessel\_segment''} table which contains \textbf{eight} different segment values, and \textbf{107} different combinations of segments and sub-segments. The provided \textit{``ports''} data contains \textbf{5200} ports world-wide that all follows the \gls{locode} naming standard. In total, as of March 2021, there were \textbf{6.4} million vessel transitions in the \textit{``vessel\_transitions''} table which was used to construct voyages. This data formed the initial data foundation for the final processed \acrfull{ml} training dataset. All the data that was copied and processed from \acrshort{mo}'s databases were processed in batches. Ports, segments, and transitions where quickly copied and processed, however, the \textbf{1.2} billion positional records took several days to migrate and validate. This was mostly because of the time required to validate coordinates and correctly map \acrshort{mmsi} and \acrshort{imo} values. Throughout this process, the latest identifiers and timestamps were fetched from the dedicated project database to only update data that occurred after the latest records already processed. In this way, this process was idempotent so that running the process multiple times did not affect existing data. This made the system simple to update throughout the development process and as many records as possible were used in the final approach only limited by the thesis time limitation.

\subsection{Voyage definition and construction}

Based on the initial \textbf{6.4} million vessel transitions, \textbf{1.7} million voyages where initially constructed by finding positional records transmitted from a vessel between subsequent departure and arrival transitions. The resulting voyages were, therefore, defined based on transitioning \acrshort{ais} statuses that indicate the vessel is moored or moving. As a consequence of this definition, the quality of the resulting trajectories are very much affected by how well the \gls{aivdm} protocol is followed by the traveling vessels. Since the navigational status attribute is manually inputted by the vessel's captain or crew, the resulting trajectories are prone to human error but results in more complete voyages disregarding intermediate stops for purposes such as bunkering.

As an example, \cref{fig:transition_voyage} shows a voyage from China to Argentina where the vessel stopped at Singapore, most likely to bunker. In the choses voyage definition, the beginning and end of the voyage is defined based on input from the vessel's captain which results in a voyage starting from China, and ending in Argentina. Using an alternative method based on clustering (\cref{sec:vessel_voyage_definition}), this voyage would have been divided into two voyages; one from China to Singapore, and one from Singapore to Argentina. Further advantages and disadvantages is later discussed in \cref{chap:discussion}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/results/voyage_quality}
    \caption{Transition voyage from China to Argentina that visits the port of Singapore exemplifying the properties of the chosen voyage definition.}
    \label{fig:transition_voyage}
\end{figure}

The \textbf{1.7} million voyages constructed using the vessel transitions were sampled based on 6 hour intervals and collected in ``sampled\_transition\_voyages'' that formed the foundation for trajectory similarity measurements. In the process of constructing the final dataset, these sampled voyages were divided into multiple incomplete voyages up to a factor of four. The resulting training dataset collected in the table ``ml\_training\_data'' consisted of \textbf{4.3} million voyages.

\subsection{Trajectory similarity and MSTD}

Using the foundation of the sampled trajectories, each trajectory was compared to every other trajectory departing the same port to calculate the \acrfull{mstd}. The \acrshort{mstd} value was used primarily as a method of abstracting geographical trajectories into categorical and numerical values that a \acrfull{ml} model could work with. This process converted a voyage's geographical trajectory into MSTD, the similarity value to the most similar trajectory, and trajectory length. Thus, the MSTD value served as an initial prediction purely based on geographical trajectory similarity measurements using \acrfull{sspd}. The \acrshort{sspd} method was chosen for its ability to effectively handle different lengths and shapes of trajectories when estimating similarity. Furthermore, in the approach proposed in \cite{Zhang2020AISApproach} the \acrshort{sspd} method performed the best out of the algorithmic approaches evaluated, although, their own Random Forest (RF) based approach performed the best. However, the way the training data is structured, the trajectory similarity method of choice is completely interchangeable with others. The only requirement for a given trajectory similarity measurement is that it also produces a similarity value that serves as a weight for the \acrshort{mstd} value.

\acrshort{mstd} as an initial prediction seemed to be a decent initial indicator as to where the vessel would be arriving. In total, there were \textbf{4 306 271} entries in the final training data generated where exactly \textbf{1 423 476} of which has the same arrival port and \acrshort{mstd} value. Thus, it can be assumed that the purely spatial prediction using incomplete sampled historical voyages based on \acrshort{sspd} was \textit{33\%} accurate. In other words, when using an algorithmic prediction approach based on purely spatial trajectory similarity measurements, voyage destinations can be predicted correctly one third of the time. This formed a baseline accuracy to beat with the \acrshort{ml}-based solution.

\subsection{ML data preparation}

After the final training dataset was built, it was discovered that in terms of arrival port frequencies, the dataset was imbalanced thus making it harder for \acrshort{ml} models to learn. Although some models can better handle dataset imbalance, a sampling approach was used to balance the dataset before training to support different ML models. Several different sampling approaches were evaluated, however, the traditional over and under -sampling methods either produced massive amounts of synthetic data, or removed almost all the original data which was shown in \cref{fig:all_samplers} in \cref{chap:method}. Thus, an ensemble sampling method of majority undersampling and ``SMOTE+ENN'' was employed to balance the dataset before training. \cref{fig:ensemble_sampler} shows the results from the ensemble sampling method that uses a combination of under and over -sampling techniques. As \cref{fig:ensemble_sampler} shows, using a subset of the full dataset, the final result is 8\% smaller than the original dataset, is a lot more balanced, but still has differences in class frequencies which persisted from the original dataset.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/imbalance/ensemble}
    \caption{Final ensemble sampling method (right) compared to original dataset (left) where the final ensemble produces a dataset similar in size to that of the original.}
    \label{fig:ensemble_sampler}
\end{figure}

\section{Model training and prediction performance}

After the training dataset was constructed, encoded, and balanced, the model was trained using different approaches as described in \cref{sec:training_process}. This section describes the results from the training processes, the final approach used, and the resulting model's performance and predictions resulting from the evaluation process described in \cref{sec:evaluation_process}.

\subsection{Training process}

As described in \cref{sec:training_process}, multiple training processes were evaluated in order to find the most appropriate method of training a larger model on an extensive dataset. For the \acrfull{xgb} model, three different training processes were evaluated in this process.

First, the iterative approach was evaluated by training the model in batches of \textit{600 000} samples at the time. This approach seemed to work as intended, however, it was discovered that during subsequent training batches, the performance of the model dropped off for each iteration. It seemed as if the model did not handle continuous training of the same model as well as it does when training one model from scratch using the complete dataset. Furthermore, the parameter \textit{``early\_stopping\_rounds''} was used in the other approaches as a method of telling the model to stop training if it does not see any improvements after the given number of rounds. When this parameter is set using the iterative approach, the model can stop producing new trees before it has constructed the total number of trees allowed by the \textit{``n\_estimators''} parameter. Since the first iteration can produce a model with less trees than allowed, the next iteration fails as the number of allowed trees does not match with the previous model's actual number of trees. Although there are ways around this issue, as using the early stopping rounds parameter is useful to avoid overfitting, the iterative approach did not seem the most appropriate during the development process.

Next, it was attempted to train the model using the external memory, or ``out-of-core'' memory version of \acrshort{xgb}. In this approach, the \acrshort{xgb} library is provided a \textit{libsvm} file which it converts to an optimized matrix format which is kept on the computers file system. However, all attempts at training the model using external memory were unsuccessful as the training process consumed all of the running computer's available memory and resulted in a ``bad allocation`` memory error. There seems to either be a misconfiguration or an underlying issue with the Python library used in the implementation. However, since the expected results from this approach should be the same as training the model in one iteration on a capable computer, these issues were not further looked into, although, it could be beneficial to reduce the resource requirements for the training process for future use. Therefore, it could warrant more investigation for future work.

Finally, the entire dataset was used to train the final model in one iteration on a computer capable of running the process. The training process ran over the course of two days and consistently required around 200GB of memory. The vast memory consumption could be somewhat reduced by not evaluating the model during the training process which is appropriate for future training processes after the model has been trained and the training configuration has been validated. As described in \cref{sec:training_process}, an extra copy of the training and test datasets were kept in memory to continuously evaluate and monitor the training process.

\subsection{Performance}

During the training process, the performance of the model was continuously evaluated to measure logarithmic loss and multi-class classification error. \cref{fig:eval_set} shows these metrics plotted over each boosting round in the training process. Both graphs starts converging at 100 decision trees have been constructed at around \textbf{1.5} log loss, and around \textbf{0.3} classification error. This corresponds to around \textbf{70\%} accuracy. Since the graphs have not completely converged, it is possible to either increase the learning rate parameter or increase the number of estimators in the tree, although it seems as if the graphs are very close to converging, so it might not increase performance noticeably and increases risk of overfitting. As there is very little difference between the performance on the training set and evaluation set, it indicates that the model is not overfitting, however, it might indicate that the model is over optimistic. This could occur when there are several similar samples in the training and the test datasets and could be a consequence of the sampling techniques used to balance the dataset. \todo{perhaps move this to discussion}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/results/eval_set}
    \caption{Logarithmic loss and classification error metrics tracked per boosting round in the training process.}
    \label{fig:eval_set}
\end{figure}

After the training process finished, the test dataset was used to make predictions to further evaluate the results. From the resulting predictions, accuracy was calculated to be \textbf{72\%}, and a class report was generated that shows more metrics for each possible class, or encoded arrival port, that might provide more insight into the model's performance than accuracy. \cref{lst:class_report} shows a summarized output from this class report showing the metrics precision, recall, f1-score, and support for each class as well as the aggregated mean values from all of the classes. As mentioned in \cref{sec:model_evaluation}, f1-score is based on precision and recall and is particularly appropriate for measuring performance on imbalanced datasets, and as \cref{lst:class_report} shows, the f1-score does not deviate much from the estimated accuracy of \textbf{72\%}, or \textbf{0.72}. This indicates that the accuracy value is reliable and is not biased by dataset imbalance.

\begin{lstlisting}[
    caption={Class report based on prediction results from the test dataset. The performance of the classifier is evaluated per class by using precition, recall, f1-score, and support.. \todo{update numbers}},
    label=lst:class_report,
    showstringspaces=false,
    basicstyle=\ttfamily,
]
[XGBoostClassifier] Class Report:
             precision    recall  f1-score   support      pred
0             0.378049  0.240310  0.293839     258.0     164.0
1             0.816850  0.810909  0.813869     275.0     273.0
2             0.722222  0.541667  0.619048     312.0     234.0
3             0.672727  0.377551  0.483660     294.0     165.0
...           ...       ...       ...          ...       ...
3067          0.824675  0.849498  0.836903     299.0     308.0
3068          0.833922  0.778878  0.805461     303.0     283.0
3069          0.773050  0.762238  0.767606     286.0     282.0
3070          0.614035  0.557325  0.584307     314.0     285.0
...           ...       ...       ...          ...       ...
avg / total   0.718698  0.715150  0.712737  878049.0  878049.0

\end{lstlisting}

Lastly, in order to ensure the model is not overfitted, a three-fold cross validation process was employed. \cref{lst:cv_result} shows the results from the three folds that the model was trained on. It is recommended, or common to use more folds ranging from five to 10, however, because of the long training time and time limitations, only three folds were used. As described in \cref{sec:model_evaluation}, since the standard deviation (noted as ``std. dev.'' in \cref{lst:cv_result}) is low, the model is likely to not be overfitted.

\begin{lstlisting}[
    caption={Output from 3-fold cross validation. \todo{update numbers}},
    label=lst:cv_result,
    showstringspaces=false,
    basicstyle=\ttfamily,
]
 Folds:      [0.70569399 0.71297481 0.72244745]
 Mean:       0.7137054183485277
 Std. dev.:  0.006859056778768982
\end{lstlisting}

\section{Prediction results}

After the model was trained and evaluated, \textit{20\%} of the total training dataset was used to evaluate the model. This evaluation process resulted in around \textit{880 000} example predictions. These predictions were further analyzed to discuss the impact and meaning of the different features used in the dataset. These results are presented in this section.

\subsection{Feature importances}

An added benefit of using a tree based model such as the \acrfull{xgb} or \acrfull{rf} model is that they can provide insight into the importances of features, or attributes. In a decision tree based ensemble, when constructing a tree, the training data is analyzed to find the best features to make splits, or branches, in the trees. After the training process, the models can then produce a ranking over what features best divided the dataset best. This is referred to as feature importance.

\begin{table}[htbp]
    \centering
    \begin{tabularx}{0.6\textwidth}{X X}
        \bfseries{Feature} & \bfseries{Importance} \\ \toprule
        sspd\_mstd         & 0.443659 \\ \midrule
        departure\_port    & 0.226288 \\ \midrule
        segmentation       & 0.180907 \\ \midrule
        sspd\_dist         & 0.083816 \\ \midrule
        trajectory\_length & 0.065331 \\ \bottomrule
    \end{tabularx}
    \caption{Feature importances based on the \acrshort{xgb} decision tree ensemble process. \todo{update numbers}}\label{tab:feature_importances}
\end{table}

\cref{tab:feature_importances} shows an overview of the produced feature importances after the \acrshort{xgb} training process. As it shows, the most important feature was the \acrshort{mstd} value at a ranking of 0.44 out of 1.0, followed by the vessel's departure port, segmentation value, and then the similarity value and voyage length. This analysis can further help decide if features are worth dropping from the dataset, and insight into what attributes are good indicators during voyage predictions.

\begin{itemize}
    \item Feature importances
    \item Impact of segment + sub-segment
    \item Impact of MSTD
\end{itemize}

\begin{lstlisting}[
    caption={Mean values of similarity value and trajectory length for correct and incorrect predictions.},
    label=lst:dist_length_impact,
    showstringspaces=false,
    tabsize=1,
    basicstyle=\ttfamily,
]
mean sspd_dist for correct predictions:           100516.78046218488
mean trajectory_length for correct predictions:   14.785189075630251

mean sspd_dist for erroneous predictions:         108496.74400684932
mean trajectory_length for erroneous predictions: 15.434931506849315
\end{lstlisting}

\subsection{Segment predictability}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/results/segment_accuracy}
    \caption{Accuracy of predictions from test set per segment.}
    \label{fig:segment_accuracy}
\end{figure}

\begin{itemize}
    \item What segments were easy to predict
    \item What sub-segments were easy to predict
\end{itemize}

\section{Real-life applications}

\subsection{Stand-alone usability}

\subsection{Integration with \acrfull{mo}}

\subsection{External evaluation from real shipping actors}

\begin{itemize}
    \item Caveats/use-cases and value
    \item Evaluation from MO
    \item Evaluation from external actors
\end{itemize}
